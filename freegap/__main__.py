import argparse
import os
import subprocess
import difflib
import logging
import json
import numpy as np
import matplotlib
import shutil
import re
import coloredlogs
from freegap.adaptivesvt import adaptive_sparse_vector, \
    top_branch, top_branch_precision, middle_branch, middle_branch_precision, precision, f_measure, \
    above_threshold_answers, remaining_epsilon, plot as plot_adaptive
from freegap.adaptive_estimates import adaptive_estimates, \
    mean_square_error as adaptive_mse, plot as plot_adaptive_estimates
from freegap.gapestimates import gap_svt_estimates, gap_topk_estimates, mean_square_error, plot as plot_estimates
from freegap.evaluate import evaluate

matplotlib.use('PDF')


# change the matplotlib settings
matplotlib.rcParams['text.usetex'] = True
matplotlib.rcParams['text.latex.preamble'] = \
    r'\usepackage{libertine}\usepackage[libertine]{newtxmath}\usepackage{sfmath}\usepackage[T1]{fontenc}'

coloredlogs.install(level='INFO', fmt='%(asctime)s %(levelname)s - %(name)s %(message)s')

logger = logging.getLogger(__name__)


def compress_pdfs(files):
    """Compress the generated PDFs. For some reason the PDFs generated by matplotlib is huge (>5M), so we will use
    ghostscript to compress the final PDFs for inclusion in the paper.
    """
    logger.info('Compressing generated PDFs...')
    if shutil.which('gs'):
        for file in files:
            os.rename(file, f'{file}.temp')
            subprocess.call(
                ['gs', '-sDEVICE=pdfwrite', '-dCompatibilityLevel=1.4', '-dPDFSETTINGS=/default', '-dNOPAUSE',
                 '-dQUIET', '-dBATCH', f'-sOutputFile={file}', f'{file}.temp']
            )
            os.remove(f'{file}.temp')
    else:
        logger.warning('Cannot find Ghost Script executable \'gs\', failed to compress produced PDFs.')


def process_datasets(folder):
    logger.info('Loading datasets')
    dataset_folder = os.path.abspath(folder)
    split = re.compile(r'[;,\s]\s*')

    for filename in os.listdir(dataset_folder):
        item_sets, records = [], 0
        if filename.endswith('.dat'):
            with open(os.path.join(dataset_folder, filename), 'r') as in_f:
                for line in in_f.readlines():
                    line = line.strip(' ,\n\r')
                    records += 1
                    for ch in split.split(line):
                        item_sets.append(ch)
            item_sets = np.unique(np.asarray(item_sets, dtype=np.int), return_counts=True)
            logger.info(f'Statistics for {filename}: # of records: {records} and # of Items: {len(item_sets[0])}')
            res = item_sets[1]
            np.random.RandomState(0).shuffle(res)
            yield os.path.splitext(filename)[0], res


def main():
    algorithm = ('All', 'AdaptiveSparseVector', 'AdaptiveEstimates', 'GapSparseVector', 'GapTopK')

    arg_parser = argparse.ArgumentParser(description=__doc__)
    arg_parser.add_argument('algorithm', help=f'The algorithm to evaluate, options are {algorithm}.')
    arg_parser.add_argument('--datasets', help='The datasets folder', required=False)
    arg_parser.add_argument('--output', help='The output folder', required=False,
                            default=os.path.join(os.curdir, 'output'))
    arg_parser.add_argument('--clear', help='Clear the output folder', required=False, default=False,
                            action='store_true')
    arg_parser.add_argument('--counting', help='Set the counting queries', required=False, default=False,
                            action='store_true')
    results = arg_parser.parse_args()

    if results.counting:
        logger.info('Counting queries flag set, evaluating on counting queries case')

    if results.counting:
        def svt_theoretical(x):
            return 1 / (1 + ((np.power(1 + np.power(x, 2.0 / 3), 3)) / (x * x)))

        def topk_theoretical(x):
            return (x - 1) / (2 * x)
    else:
        def svt_theoretical(x):
            return 1 / (1 + ((np.power(1 + np.power(2 * x, 2.0 / 3), 3)) / (x * x)))

        def topk_theoretical(x):
            return (x - 1) / (5 * x)

    parameters = {
        'AdaptiveSparseVector': (adaptive_sparse_vector, (top_branch, top_branch_precision, middle_branch,
                                                          middle_branch_precision, precision, f_measure,
                                                          above_threshold_answers, remaining_epsilon), plot_adaptive, {}),
        'GapSparseVector': (gap_svt_estimates, (mean_square_error,), plot_estimates,  {
            'theoretical': svt_theoretical,
            'algorithm_name': 'Sparse Vector with Measures'
        }),
        'GapTopK': (gap_topk_estimates, (mean_square_error,), plot_estimates, {
            'theoretical': topk_theoretical,
            'algorithm_name': 'Noisy Top-K with Measures'
        }),
        'AdaptiveEstimates': (adaptive_estimates, (adaptive_mse,), plot_adaptive_estimates, {})
    }

    # default value for datasets path
    results.datasets = os.path.join(os.path.curdir, 'datasets') if results.datasets is None else results.datasets

    winning_algorithm = algorithm[
        np.fromiter((difflib.SequenceMatcher(None, results.algorithm, name).ratio() for name in algorithm),
                    dtype=np.float).argmax()
    ]

    winning_algorithm = algorithm[1:] if winning_algorithm == 'All' else (winning_algorithm, )
    output_folder = os.path.abspath(results.output)

    for algorithm_name in winning_algorithm:
        # create the algorithm output folder if not exists
        algorithm_folder = os.path.join(output_folder, f'{algorithm_name}-counting') if results.counting else \
            os.path.join(output_folder, algorithm_name)

        if results.clear:
            logger.info('Clear flag set, removing the algorithm output folder...')
            shutil.rmtree(algorithm_folder, ignore_errors=True)
        os.makedirs(algorithm_folder, exist_ok=True)

        for dataset in process_datasets(results.datasets):
            # evaluate the algorithms and plot the figures
            evaluate_algorithm, metrics, plot, kwargs = parameters[algorithm_name]
            k_array = np.fromiter(range(2, 25), dtype=np.int)

            # check if result json is present (so we don't have to run again)
            # if --clear flag is specified, output folder will be empty, thus won't cause problem here
            json_file = os.path.join(algorithm_folder, f'{algorithm_name}-{dataset[0]}.json')
            if os.path.exists(json_file):
                logger.info('Found stored json file, loading...')
                with open(json_file, 'r') as fp:
                    data = json.load(fp)
            else:
                logger.info('No json file exists, running experiments...')
                if 'Gap' in algorithm_name:
                    # we need to run multiple epsilons to draw fix-k figures for gap estimates algorithms
                    epsilons = tuple(n / 10 for n in range(1, 16))
                else:
                    epsilons = (0.7, )
                data = evaluate(evaluate_algorithm, dataset, metrics=metrics, epsilons=epsilons, k_array=k_array,
                                counting_queries=results.counting)
                logger.info('Dumping data into json file...')
                with open(json_file, 'w') as fp:
                    json.dump(data, fp)
            logger.info('Plotting')
            generated_files = plot(k_array, dataset[0], data, algorithm_folder, **kwargs)
            compress_pdfs(generated_files)


if __name__ == '__main__':
    main()

